# ğŸ¤– LLM from Scratch Project

This project is based on the "Build a Large Language Model from Scratch" book by Sebastian Raschka. The notebooks have been converted into a modular Python project that can be run with `python main.py`. ğŸ“š

## Important notes 
![Quality Characteristics](images/quality_characteristics.png)

*Comparison of different model characteristics and quality metrics*


## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py              # Main script to run demos
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ tokenization.py      # Tokenization utilities (Chapter 2)
â”œâ”€â”€ attention.py         # Attention mechanisms (Chapter 3)
â”œâ”€â”€ gpt_model.py        # GPT model implementation (Chapter 4)
â”œâ”€â”€ training_utils.py   # Training utilities (Chapter 5)
â”œâ”€â”€ classification.py   # Classification utilities (Chapter 6)
â”œâ”€â”€ README.md           # This file
â””â”€â”€ ch*.ipynb          # Original Jupyter notebooks
```

## âš¡ Installation

1. Install dependencies:
```bash
uv pip install -r requirements.txt
```

## ğŸš€ Usage

### ğŸŒ Run Streamlit Web Interface:
```bash
streamlit run app.py
```

This launches an interactive web interface where you can:
- Generate text using pretrained GPT-2 models (Small, Medium, Large, XL)
- Adjust generation parameters (temperature, max tokens, top-k sampling)
- Try example prompts to understand GPT-2 behavior
- Switch between different model sizes

![Streamlit Interface](images/prompt.png)

*The interactive web interface showing the prompt input and model selection options*

![Model Results Comparison](images/small_result.png)

*Example output from GPT-2 Small model*

![Large Model Results](images/large_result.png)

*Example output from GPT-2 Large model showing improved quality*


### ğŸ’» Run Command Line Demos:

#### Run all demos:
```bash
python main.py
```

#### Run specific demos:
```bash
python main.py --demo tokenization    # Tokenization demo
python main.py --demo model          # Model creation demo
python main.py --demo generation     # Text generation demo
python main.py --demo training       # Training setup demo
python main.py --demo classification # Classification setup demo
```

## âœ¨ Features

## âš™ï¸ Model Configurations

The project includes two pre-configured models:

- **gpt2-small**: 12 layers, 768 embedding dimensions (similar to GPT-2 124M)
- **gpt2-mini**: 6 layers, 384 embedding dimensions (smaller for quick testing)


## ğŸ“– Original Source

Based on the book "Build a Large Language Model From Scratch" by Sebastian Raschka.
- Book: http://mng.bz/orYv
- Original code: https://github.com/rasbt/LLMs-from-scratch

## ğŸ“‹ Requirements

- Python 3.8+
- PyTorch 2.0+
- Other dependencies listed in requirements.txt

## ğŸ“ Notes

- The models are simplified for educational purposes
- Training data is minimal (short story) for quick experimentation
- For production use, you would need larger datasets and more compute resources
- GPU is recommended but not required for the demos
